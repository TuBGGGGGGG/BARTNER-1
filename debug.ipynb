{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "x = [1,2,3]\n",
    "z=[1]\n",
    "y=[]\n",
    "for i in range(10000):\n",
    "    y.append(x)\n",
    "    x+=z\n",
    "time1 = time.time()\n",
    "target_spans = set([str(span) for span in y])\n",
    "print(time.time()-time1)\n",
    "time1 = time.time()\n",
    "result_set = set(map(str, y))\n",
    "print(time.time()-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from data.pipe import BartNERPipe\n",
    "from model.bart import BartSeq2SeqModel\n",
    "import fitlog\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from fastNLP import Trainer\n",
    "from model.metrics import Seq2SeqSpanMetric\n",
    "from model.losses import Seq2SeqLoss\n",
    "from torch import optim\n",
    "from fastNLP import BucketSampler, GradientClipCallback, cache_results\n",
    "\n",
    "from model.callbacks import WarmupCallback\n",
    "from fastNLP.core.sampler import SortedSampler\n",
    "from model.generater import SequenceGeneratorModel\n",
    "from fastNLP.core.sampler import  ConstTokenNumSampler\n",
    "from model.callbacks import FitlogCallback\n",
    "\n",
    "fitlog.debug()\n",
    "fitlog.set_log_dir('logs')\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset_name', default='en_ace04', type=str)\n",
    "\n",
    "def set_seed(seed=1996):\n",
    "    print(\"[SET SEED]: \",seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "args= parser.parse_args([])\n",
    "dataset_name = args.dataset_name\n",
    "args.length_penalty = 1\n",
    "args.save_model = 1\n",
    "\n",
    "# word: 生成word的start; bpe: 生成所有的bpe; span: 每一段按照start end生成; span_bpe: 每一段都是start的所有bpe，end的所有bpe\n",
    "args.target_type = 'word'\n",
    "args.bart_name = '/disk1/wxl/Desktop/DeepKE/example/ner/huggingface/bart-large'\n",
    "args.schedule = 'linear'\n",
    "args.decoder_type = 'avg_feature'\n",
    "args.n_epochs = 30\n",
    "args.num_beams = 1\n",
    "args.batch_size = 16\n",
    "args.use_encoder_mlp = 1\n",
    "args.lr = 1e-5\n",
    "args.warmup_ratio = 0.01\n",
    "eval_start_epoch = 1\n",
    "\n",
    "# the following hyper-parameters are for target_type=word\n",
    "if dataset_name == 'conll2003':  # three runs get 93.18/93.18/93.36 F1\n",
    "    max_len, max_len_a = 10, 0.6\n",
    "elif dataset_name == 'en-ontonotes':  # three runs get 90.46/90.4/90/52 F1\n",
    "    max_len, max_len_a = 10, 0.8\n",
    "elif dataset_name == 'CADEC':\n",
    "    max_len, max_len_a = 10, 1.6\n",
    "    args.num_beams = 4\n",
    "    args.lr = 2e-5\n",
    "    args.n_epochs = 30\n",
    "    eval_start_epoch=10\n",
    "elif dataset_name == 'Share_2013':\n",
    "    max_len, max_len_a = 10, 0.6\n",
    "    args.use_encoder_mlp = 0\n",
    "    args.num_beams = 4\n",
    "    args.lr = 2e-5\n",
    "    eval_start_epoch = 5\n",
    "elif dataset_name == 'Share_2014':\n",
    "    max_len, max_len_a = 10, 0.6\n",
    "    args.num_beams = 4\n",
    "    eval_start_epoch = 5\n",
    "    args.n_epochs = 30\n",
    "elif dataset_name == 'genia':  # three runs: 79.29/79.13/78.75\n",
    "    max_len, max_len_a = 10, 0.5\n",
    "    args.target_type = 'span'\n",
    "    args.lr = 2e-5\n",
    "    args.warmup_ratio = 0.01\n",
    "elif dataset_name == 'en_ace04':  # four runs: 86.84/86.33/87/87.17\n",
    "    max_len, max_len_a = 50, 1.1\n",
    "    args.n_epochs = 55\n",
    "    args.batch_size = 48\n",
    "    args.lr = 4e-5\n",
    "    seed = 4373\n",
    "elif dataset_name == 'en_ace05':  # three runs: 85.39/84.54/84.75\n",
    "    max_len, max_len_a = 50, 0.7\n",
    "    args.lr = 3e-5\n",
    "    args.batch_size = 12\n",
    "    args.num_beams = 4\n",
    "    args.warmup_ratio = 0.1\n",
    "\n",
    "set_seed(seed)\n",
    "# with open(\"/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/loss_log/D.json\",\"r\") as f:\n",
    "#     b=f.readlines()\n",
    "# with open(\"/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/loss_log/E.json\",\"r\") as f:\n",
    "#     c=f.readlines()\n",
    "# for x,y in zip(b,c):\n",
    "#     assert x==y,print(x,y)\n",
    "# exit()\n",
    "\n",
    "save_model = args.save_model\n",
    "del args.save_model\n",
    "lr = args.lr\n",
    "n_epochs = args.n_epochs\n",
    "batch_size = args.batch_size\n",
    "num_beams = args.num_beams\n",
    "\n",
    "length_penalty = args.length_penalty\n",
    "if isinstance(args.decoder_type, str) and args.decoder_type.lower() == 'none':\n",
    "    args.decoder_type = None\n",
    "decoder_type = args.decoder_type\n",
    "target_type = args.target_type\n",
    "bart_name = args.bart_name\n",
    "schedule = args.schedule\n",
    "use_encoder_mlp = args.use_encoder_mlp\n",
    "\n",
    "fitlog.add_hyper(args)\n",
    "\n",
    "#######hyper\n",
    "#######hyper\n",
    "\n",
    "demo = False\n",
    "if demo:\n",
    "    cache_fn = f\"caches/data_{bart_name}_{dataset_name}_{target_type}_demo.pt\"\n",
    "else:\n",
    "    cache_fn = f\"caches/data_{bart_name}_{dataset_name}_{target_type}.pt\"\n",
    "\n",
    "@cache_results(cache_fn, _refresh=False)\n",
    "def get_data():\n",
    "    pipe = BartNERPipe(tokenizer=bart_name, dataset_name=dataset_name, target_type=target_type)\n",
    "    if dataset_name == 'conll2003':\n",
    "        paths = {'test': \"./data/conll2003/test.txt\",\n",
    "                 'train': \"./data/conll2003/train.txt\",\n",
    "                 'dev': \"./data/conll2003/dev.txt\"}\n",
    "        data_bundle = pipe.process_from_file(paths, demo=demo)\n",
    "    elif dataset_name == 'en-ontonotes':\n",
    "        paths = './data/en-ontonotes/english'\n",
    "        data_bundle = pipe.process_from_file(paths)\n",
    "    else:\n",
    "        data_bundle = pipe.process_from_file(f'./data/{dataset_name}', demo=demo)\n",
    "    return pipe, data_bundle, pipe.tokenizer, pipe.mapping2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, data_bundle, tokenizer, mapping2id = get_data()\n",
    "ds = data_bundle.get_dataset(\"train\")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "is_key = set()\n",
    "for id,i in enumerate(ds):\n",
    "    # x = []\n",
    "    # for j in i[\"target_span\"]:\n",
    "        \n",
    "    #     if j not in x:\n",
    "    #         x.append(j)\n",
    "    #     else:\n",
    "    #         print(i[\"tgt_tokens\"])\n",
    "    #         print(i[\"raw_words\"])\n",
    "    #         print(id)\n",
    "    if len(i[\"target_span\"])==0:\n",
    "        is_key.add(str(i[\"raw_words\"]))\n",
    "print(len(is_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy \n",
    "from fastNLP import DataSet\n",
    "def current_tree_init(ds):\n",
    "    num = 0\n",
    "    num2 = 0\n",
    "    num_x = 0\n",
    "    num_zero_ent = 0\n",
    "    ds_ = DataSet()\n",
    "\n",
    "    current_tree = {}\n",
    "\n",
    "    is_ordered_key = set()\n",
    "\n",
    "    split_list = [\"entities\",\"entity_tags\",\"entity_spans\",\"tgt_tokens\",\"target_span\",\"tgt_seq_len\"]\n",
    "    \n",
    "    for id, ds_i in enumerate(ds):\n",
    "        before = {}\n",
    "\n",
    "        for i in split_list:\n",
    "            before[i]=ds_i[i]\n",
    "\n",
    "        num2 += len(before[\"entity_tags\"])\n",
    "        key = ' '.join(ds_i[\"raw_words\"])\n",
    "        if key in current_tree.keys():\n",
    "            continue # 重复数据\n",
    "        value = {\n",
    "            \"ds_indexes\": [],\n",
    "            \"position\": 1,\n",
    "            \"stop_position\": ds_i[\"tgt_seq_len\"]\n",
    "        }\n",
    "        '''\n",
    "            [position]: 未排序的第一个位置\n",
    "            [stop_position]: 排序结束的位置，初始化为key对应的原始tgt_seq_len\n",
    "        '''\n",
    "        \n",
    "        num+=len(before[\"entity_tags\"])\n",
    "        if len(before[\"entity_tags\"]) <=1:\n",
    "            num_x += 1 \n",
    "\n",
    "        for i in range(len(before[\"entity_tags\"])):\n",
    "            split_i = {\n",
    "                \"entities\":before[\"entities\"][i:i+1],\n",
    "                \"entity_tags\":before[\"entity_tags\"][i:i+1],\n",
    "                \"entity_spans\":before[\"entity_spans\"][i:i+1],\n",
    "                \"target_span\":before[\"target_span\"][i:i+1]\n",
    "            }\n",
    "            split_i[\"tgt_tokens\"] = [0] + split_i[\"target_span\"][0] + [1]\n",
    "            split_i[\"tgt_seq_len\"] = len(split_i[\"tgt_tokens\"])\n",
    "\n",
    "            x = deepcopy(ds_i)\n",
    "            for j in split_list:\n",
    "                x[j] = split_i[j]\n",
    "\n",
    "            ds_.append(x)\n",
    "\n",
    "            value[\"ds_indexes\"].append(len(ds_)-1)\n",
    "\n",
    "        if len(value[\"ds_indexes\"]) <= 1:\n",
    "            value[\"position\"] = value[\"stop_position\"] # 0个或1个实体，不用排序\n",
    "            is_ordered_key.add(key)\n",
    "            if len(value[\"ds_indexes\"]) == 0:\n",
    "                num_zero_ent += 1\n",
    "                x = deepcopy(ds_i)\n",
    "                ds_.append(x) # 0个实体，直接将原数据加回去\n",
    "                value[\"ds_indexes\"].append(len(ds_)-1)\n",
    "        current_tree[key] = value # 处理完之后加入字典树\n",
    "        \n",
    "\n",
    "    print(\"非去重实体个数\",num2,\" 去重实体个数： \",num,\" 新训练数据个数：\",len(ds_),\" 不重复数据个数: \", len(current_tree), \" 0实体数据个数：\",num_zero_ent)\n",
    "    print(num_x)\n",
    "    assert len(ds_) == num+num_zero_ent\n",
    "    return ds_, current_tree, is_ordered_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_,current_tree, is_ordered_key = current_tree_init(ds)\n",
    "print(len(ds_))\n",
    "ds_.set_ignore_type('target_span', 'entities')\n",
    "ds_.set_pad_val('tgt_tokens', 1)  # 设置为eos所在的id\n",
    "ds_.set_pad_val('src_tokens', pipe.tokenizer.pad_token_id)\n",
    "\n",
    "ds_.apply_field(lambda x: len(x), field_name='src_tokens', new_field_name='src_seq_len')\n",
    "ds_.apply_field(lambda x: len(x), field_name='tgt_tokens', new_field_name='tgt_seq_len')\n",
    "ds_.set_input('tgt_tokens', 'src_tokens', 'src_seq_len', 'tgt_seq_len', 'first')\n",
    "ds_.set_target('tgt_tokens', 'tgt_seq_len', 'target_span', 'entities')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'max_len_a:{max_len_a}, max_len:{max_len}')\n",
    "\n",
    "print(data_bundle)\n",
    "print(\"The number of tokens in tokenizer \", len(tokenizer.decoder))\n",
    "\n",
    "bos_token_id = 0\n",
    "eos_token_id = 1\n",
    "label_ids = list(mapping2id.values())\n",
    "model = BartSeq2SeqModel.build_model(bart_name, tokenizer, label_ids=label_ids, decoder_type=decoder_type,\n",
    "                                     use_encoder_mlp=use_encoder_mlp)\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "print(vocab_size, model.decoder.decoder.embed_tokens.weight.data.size(0))\n",
    "model = SequenceGeneratorModel(model, bos_token_id=bos_token_id,\n",
    "                               eos_token_id=eos_token_id,\n",
    "                               max_length=max_len, max_len_a=max_len_a,num_beams=num_beams, do_sample=False,\n",
    "                               repetition_penalty=1, length_penalty=length_penalty, pad_token_id=eos_token_id,\n",
    "                               restricter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=torch.load(\"/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/save_models/en_ace04_3257/best_SequenceGeneratorModel_f_2024-11-18-22-16-54-082796\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_final_data(current_tree, ds, ds2):\n",
    "    num_ent = 0\n",
    "    for ds_i in ds:\n",
    "    # 映射流程：ds[i] -> key -> current_tree_final[key] -> ds2[de_indexes[0]]\n",
    "        key = ' '.join(ds_i[\"raw_words\"])\n",
    "        ds2_i = ds2[current_tree[key][\"ds_indexes\"][0]]\n",
    "        assert len(current_tree[key][\"ds_indexes\"])==1\n",
    "    # assert: tgt_seq_len == len(tgt_tokens), tgt_tokens == [0] + Σtatget_span[i] + [1]\n",
    "        assert ds2_i[\"tgt_seq_len\"] == len(ds2_i[\"tgt_tokens\"])\n",
    "        tmp = []\n",
    "        for i in ds2_i[\"target_span\"]:\n",
    "            tmp += i\n",
    "        assert ds2_i[\"tgt_tokens\"] == [0] + tmp + [1]\n",
    "    # ds_set()/ds2_set(): str(entities[i]+entity_tags[i]+entity_spans[i]+target_span[i]) \n",
    "        ds_set = set()\n",
    "        ds2_set = set()\n",
    "        num_ent += len(ds2_i[\"entities\"]) \n",
    "        assert len(ds2_i[\"entities\"]) == len(ds2_i[\"entity_tags\"]) == len(ds2_i[\"entity_spans\"]) == len(ds2_i[\"target_span\"])\n",
    "        for i in range(len(ds2_i[\"entities\"])):\n",
    "            v = str(ds2_i[\"entities\"][i]) + str(ds2_i[\"entity_tags\"][i]) + str(ds2_i[\"entity_spans\"][i])+ str(ds2_i[\"target_span\"][i])\n",
    "            ds2_set.add(v)\n",
    "        for i in range(len(ds_i[\"entities\"])):\n",
    "            v = str(ds_i[\"entities\"][i]) + str(ds_i[\"entity_tags\"][i]) + str(ds_i[\"entity_spans\"][i])+ str(ds_i[\"target_span\"][i])\n",
    "            ds_set.add(v)\n",
    "    # 交集 == len(ds_set) == len(ds2_set)\n",
    "        inter_set = ds2_set.intersection(ds_set)\n",
    "        assert len(inter_set) == len(ds_set) == len(ds2_set)\n",
    "    return num_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastNLP import seq_len_to_mask\n",
    "from fastNLP.core.utils import _move_dict_value_to_device\n",
    "from fastNLP import SequentialSampler\n",
    "from fastNLP import DataSetIter\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_update_loss(tgt_tokens, tgt_seq_len, pred):\n",
    "    \"\"\"\n",
    "    :param tgt_tokens: bsz x max_len, 包含了的[sos, token, eos]\n",
    "    :param pred: bsz x max_len-1 x vocab_size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tgt_seq_len = tgt_seq_len - 1\n",
    "    time1 = time.time()\n",
    "    mask = seq_len_to_mask(tgt_seq_len, max_len=tgt_tokens.size(1) - 1).eq(0)\n",
    "    time_res = time.time() - time1\n",
    "    tgt_tokens = tgt_tokens[:, 1:].masked_fill(mask, -100)\n",
    "    loss = F.cross_entropy(target=tgt_tokens, input=pred.transpose(1,2), reduction='none').sum(-1) / tgt_seq_len\n",
    "    return loss,time_res\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_loss_all(ds, model):\n",
    "    # 下标或者add/delete操作会变化\n",
    "    time_sum = 0\n",
    "    sampler = SequentialSampler()\n",
    "    batch = DataSetIter(batch_size=1024, dataset=ds, sampler=sampler)\n",
    "    model.eval()\n",
    "    loss_all = torch.empty(0, device='cuda')\n",
    "    for id,i in enumerate(batch):\n",
    "        i[0][\"update_tree\"]=True\n",
    "        _move_dict_value_to_device(i[0],i[-1],device=torch.device('cuda'))\n",
    "        tgt_tokens = i[0]['tgt_tokens']\n",
    "        tgt_seq_len = i[-1]['tgt_seq_len']\n",
    "        \n",
    "        pred = model(**i[0])['pred']\n",
    "        \n",
    "        batch_loss,time_res = get_update_loss(tgt_tokens, tgt_seq_len, pred)\n",
    "        time_sum += time_res\n",
    "        loss_all = torch.cat((loss_all, batch_loss))\n",
    "    print(time_sum)\n",
    "    return loss_all\n",
    "\n",
    "@torch.no_grad()\n",
    "def updata_func(current_tree1, is_ordered_key1, ds_,loss_all, pipe):\n",
    "    # ds_是分化后的训练数据集\n",
    "    current_tree = deepcopy(current_tree1)\n",
    "    assert current_tree[list(current_tree.keys())[-1]][\"ds_indexes\"][-1] == len(ds_) - 1,f'{current_tree[list(current_tree.keys())[-1]][\"ds_indexes\"][-1]} {len(ds_) - 1}'\n",
    "    is_ordered_key = deepcopy(is_ordered_key1)\n",
    "    ds2 = DataSet()\n",
    "    split_list = [\"entities\",\"entity_tags\",\"entity_spans\",\"tgt_tokens\",\"target_span\",\"tgt_seq_len\"]\n",
    "    add_list = [\"entities\",\"entity_tags\",\"entity_spans\",\"target_span\"] # 直接将最后一个加在hard尾巴的key\n",
    "    for key in current_tree.keys():\n",
    "        if len(current_tree[key][\"ds_indexes\"]) == 1 or current_tree[key]['position'] == current_tree[key]['stop_position']:\n",
    "            assert key in is_ordered_key,f'{key},{current_tree[key],{ds_[0]}}'\n",
    "        if key in is_ordered_key:\n",
    "            # 已经排序完毕的key\n",
    "            assert len(current_tree[key]['ds_indexes']) == 1 and current_tree[key]['position'] == current_tree[key]['stop_position']\n",
    "\n",
    "            ds2.append(ds_[current_tree[key]['ds_indexes'][0]])\n",
    "            current_tree[key][\"ds_indexes\"] = [len(ds2)-1] # 更新一下数据索引\n",
    "            continue\n",
    "\n",
    "        position = current_tree[key]['position']\n",
    "        ds_indexes = current_tree[key]['ds_indexes']\n",
    "        loss_unordered = loss_all[ds_indexes]\n",
    "        hard_node_id = ds_indexes[loss_unordered.argmax()]\n",
    "        ds_hard = ds_[hard_node_id]\n",
    "        \n",
    "        hard_value = {}\n",
    "        for i in split_list:\n",
    "            hard_value[i] = ds_hard[i]\n",
    "        hard_value[\"tgt_tokens\"] = hard_value[\"tgt_tokens\"][:-1]\n",
    "\n",
    "        new_ds_indexes=[]\n",
    "\n",
    "        for id in ds_indexes:\n",
    "            if id==hard_node_id:\n",
    "                continue\n",
    "\n",
    "            add_value = {}\n",
    "            for i in add_list:\n",
    "                add_value[i] = ds_[id][i][-1:]\n",
    "\n",
    "            add_value[\"tgt_tokens\"] = ds_[id][\"tgt_tokens\"][position:]\n",
    "\n",
    "            add_ds = deepcopy(ds_hard)\n",
    "            for i in add_list:\n",
    "                add_ds[i] = hard_value[i] + add_value[i]\n",
    "            add_ds[\"tgt_tokens\"] = hard_value[\"tgt_tokens\"] + add_value[\"tgt_tokens\"]\n",
    "            add_ds[\"tgt_seq_len\"] = len(add_ds[\"tgt_tokens\"])\n",
    "\n",
    "            ds2.append(add_ds)\n",
    "\n",
    "            new_ds_indexes.append(len(ds2)-1)\n",
    "        \n",
    "        current_tree[key][\"ds_indexes\"] = new_ds_indexes\n",
    "        if len(new_ds_indexes) == 1:\n",
    "            new_position = len(ds2[-1][\"tgt_tokens\"]) # 理论上=stop_position\n",
    "            assert new_position == current_tree[key][\"stop_position\"], f'{new_position} {current_tree[key][\"stop_position\"]} {ds2[-1][\"tgt_tokens\"]}, {len(ds2)}'\n",
    "        else:\n",
    "            new_position = len(hard_value[\"tgt_tokens\"])\n",
    "\n",
    "        current_tree[key][\"position\"] = new_position\n",
    "        if new_position == current_tree[key][\"stop_position\"]:\n",
    "            assert len(new_ds_indexes)==1,f'分化数据归一才能算排序完成！！！'\n",
    "            is_ordered_key.add(key)\n",
    "        ds2.set_ignore_type('target_span', 'entities')\n",
    "        ds2.set_pad_val('tgt_tokens', 1)  # 设置为eos所在的id\n",
    "        ds2.set_pad_val('src_tokens', pipe.tokenizer.pad_token_id)\n",
    "\n",
    "        ds2.apply_field(lambda x: len(x), field_name='src_tokens', new_field_name='src_seq_len')\n",
    "        ds2.apply_field(lambda x: len(x), field_name='tgt_tokens', new_field_name='tgt_seq_len')\n",
    "        ds2.set_input('tgt_tokens', 'src_tokens', 'src_seq_len', 'tgt_seq_len', 'first')\n",
    "        ds2.set_target('tgt_tokens', 'tgt_seq_len', 'target_span', 'entities')\n",
    "    return current_tree, is_ordered_key, ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tree(ds, model, current_tree, is_ordered_key, pipe):\n",
    "    # 通过get_loss_all获得在训练集上的每一条loss\n",
    "    loss_all = get_loss_all(ds, model)\n",
    "    # 再通过updata_func获得新的ds,current_tree, is_ordered_key\n",
    "    new_current_tree, new_is_ordered_key, new_ds = updata_func(current_tree, is_ordered_key, ds,loss_all, pipe)\n",
    "    return new_current_tree, new_is_ordered_key, new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(is_ordered_key) < len(current_tree):\n",
    "    x = len(is_ordered_key)\n",
    "    current_tree, is_ordered_key, ds_ = update_tree(ds_, model2, current_tree, is_ordered_key, pipe)\n",
    "    print(f\"{x} -> {len(is_ordered_key)} max: {len(current_tree)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastNLP import DataSet,DataSetIter\n",
    "from fastNLP import SequentialSampler\n",
    "ds_ = DataSet()\n",
    "ds_ =ds.load(\"/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/caches/ds_final_11_18_2.pt\")\n",
    "sampler = SequentialSampler()\n",
    "batch = DataSetIter(batch_size=1, dataset=ds_, sampler=sampler)\n",
    "# model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = [\"entities\",\"entity_tags\",\"entity_spans\",\"tgt_tokens\",\"target_span\",\"tgt_seq_len\"]\n",
    "name_list = [\"raw_words\", \"src_tokens\",\"first\",\"src_seq_len\"]\n",
    "# num=0\n",
    "# for id,i in enumerate(ds):\n",
    "#     num+=1\n",
    "#     if len(i[\"entity_tags\"]) > -1:\n",
    "#         for j in split_list:\n",
    "#             try:\n",
    "#                 i[j].sort()\n",
    "#                 ds_[id][j].sort()\n",
    "#                 assert(i[j]==ds_[id][j])\n",
    "#             except:\n",
    "#                 assert type(i[j])==type(1) and i[j]==ds_[id][j]\n",
    "#         print(\"\\n\")\n",
    "# print(num)\n",
    "for id,i in enumerate(ds):\n",
    "    for j in name_list:\n",
    "        assert i[j] == ds_[id][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds))\n",
    "for id,i in enumerate(batch):\n",
    "    src_tokens=i[0][\"src_tokens\"]\n",
    "    src_seq_len=i[0][\"src_seq_len\"]\n",
    "    first=i[0][\"first\"]\n",
    "    if len(ds[id][\"entity_tags\"]) > 20:\n",
    "        # split_list = [\"entities\",\"entity_tags\",\"entity_spans\",\"tgt_tokens\",\"target_span\",\"tgt_seq_len\"]\n",
    "        # for j in split_list:\n",
    "            # print(j,\" \",i[j])\n",
    "        x=i[0][\"tgt_tokens\"].to('cuda')\n",
    "        # print(ds[id][\"entities\"])\n",
    "        # print(ds[id][\"entity_tags\"])\n",
    "        # print(len(ds[id][\"entities\"])==len(ds[id][\"entity_tags\"]))\n",
    "        y = model2.predict(src_tokens.to('cuda'), src_seq_len.to('cuda'), first.to('cuda'))['pred']\n",
    "        \n",
    "        if x.equal(y) != False:\n",
    "            print(ds[id][\"tgt_tokens\"])\n",
    "            print(\"tgt_tokens: \",x)\n",
    "            print(\"pred: \",y)\n",
    "        print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = list(mapping2id.values())\n",
    "metric = Seq2SeqSpanMetric(1, num_labels=len(label_ids), target_type=target_type)\n",
    "model2.eval()\n",
    "\n",
    "for i in batch:\n",
    "    src_tokens=i[0][\"src_tokens\"]\n",
    "    src_seq_len=i[0][\"src_seq_len\"]\n",
    "    first=i[0][\"first\"]\n",
    "    pred = model2.predict(src_tokens.to('cuda'), src_seq_len.to('cuda'), first.to('cuda'))['pred']\n",
    "    res = metric.evaluate(i[1][\"target_span\"], pred.cpu(), i[1][\"tgt_tokens\"])\n",
    "print(metric.get_metric())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
