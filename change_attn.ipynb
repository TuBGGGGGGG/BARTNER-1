{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybuilddir.txt\n",
      "pybuilddir.txt\n",
      "/usr/share/zoneinfo/UTC\n",
      "/usr/lib/ssl/certs/ca-certificates.crt\n",
      "[SET SEED]:  3257\n",
      "max_len_a:1.6, max_len:10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from data.pipe import BartNERPipe\n",
    "from model.bart import BartSeq2SeqModel\n",
    "import fitlog\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from fastNLP import Trainer\n",
    "from model.metrics import Seq2SeqSpanMetric\n",
    "from model.losses import Seq2SeqLoss\n",
    "from torch import optim\n",
    "from fastNLP import BucketSampler, GradientClipCallback, cache_results\n",
    "\n",
    "from model.callbacks import WarmupCallback\n",
    "from fastNLP.core.sampler import SortedSampler\n",
    "from model.generater import SequenceGeneratorModel\n",
    "from fastNLP.core.sampler import  ConstTokenNumSampler\n",
    "from model.callbacks import FitlogCallback\n",
    "\n",
    "fitlog.debug()\n",
    "fitlog.set_log_dir('logs')\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset_name', default='re_ace05', type=str)\n",
    "\n",
    "def set_seed(seed=1996):\n",
    "    print(\"[SET SEED]: \",seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "args= parser.parse_args([])\n",
    "dataset_name = args.dataset_name\n",
    "args.length_penalty = 1\n",
    "args.save_model = 1\n",
    "\n",
    "# word: 生成word的start; bpe: 生成所有的bpe; span: 每一段按照start end生成; span_bpe: 每一段都是start的所有bpe，end的所有bpe\n",
    "args.target_type = 'word'\n",
    "args.bart_name = '/disk1/wxl/Desktop/DeepKE/example/ner/huggingface/bart-large'\n",
    "args.schedule = 'linear'\n",
    "args.decoder_type = 'avg_feature'\n",
    "args.n_epochs = 30\n",
    "args.num_beams = 1\n",
    "args.batch_size = 16\n",
    "args.use_encoder_mlp = 1\n",
    "args.lr = 1e-5\n",
    "args.warmup_ratio = 0.01\n",
    "eval_start_epoch = 1\n",
    "\n",
    "# the following hyper-parameters are for target_type=word\n",
    "if dataset_name == 'conll2003':  # three runs get 93.18/93.18/93.36 F1\n",
    "    max_len, max_len_a = 10, 0.6\n",
    "elif dataset_name == 'en-ontonotes':  # three runs get 90.46/90.4/90/52 F1\n",
    "    max_len, max_len_a = 10, 0.8\n",
    "elif dataset_name == 'CADEC':\n",
    "    max_len, max_len_a = 10, 1.6\n",
    "    args.num_beams = 4\n",
    "    args.lr = 2e-5\n",
    "    args.n_epochs = 30\n",
    "    eval_start_epoch=10\n",
    "elif dataset_name == 'Share_2013':\n",
    "    max_len, max_len_a = 10, 0.6\n",
    "    args.use_encoder_mlp = 0\n",
    "    args.num_beams = 4\n",
    "    args.lr = 2e-5\n",
    "    eval_start_epoch = 5\n",
    "elif dataset_name == 'Share_2014':\n",
    "    max_len, max_len_a = 10, 0.6\n",
    "    args.num_beams = 4\n",
    "    eval_start_epoch = 5\n",
    "    args.n_epochs = 30\n",
    "elif dataset_name == 'genia':  # three runs: 79.29/79.13/78.75\n",
    "    max_len, max_len_a = 10, 0.5\n",
    "    args.target_type = 'span'\n",
    "    args.lr = 2e-5\n",
    "    args.warmup_ratio = 0.01\n",
    "elif dataset_name == 'en_ace04':  # four runs: 86.84/86.33/87/87.17\n",
    "    max_len, max_len_a = 50, 1.1\n",
    "    args.n_epochs = 55\n",
    "    args.batch_size = 48\n",
    "    args.lr = 4e-5\n",
    "    seed = 4373\n",
    "elif dataset_name == 're_ace05':\n",
    "    max_len, max_len_a = 10, 1.6\n",
    "    args.num_beams = 4\n",
    "    args.lr = 2e-5\n",
    "    args.batch_size = 80\n",
    "    args.n_epochs = 100\n",
    "    seed = 3257\n",
    "    eval_start_epoch=1\n",
    "elif dataset_name == 'en_ace05':  # three runs: 85.39/84.54/84.75\n",
    "    max_len, max_len_a = 50, 0.7\n",
    "    args.lr = 3e-5\n",
    "    args.batch_size = 12\n",
    "    args.num_beams = 4\n",
    "    args.warmup_ratio = 0.1\n",
    "\n",
    "set_seed(seed)\n",
    "# with open(\"/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/loss_log/D.json\",\"r\") as f:\n",
    "#     b=f.readlines()\n",
    "# with open(\"/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/loss_log/E.json\",\"r\") as f:\n",
    "#     c=f.readlines()\n",
    "# for x,y in zip(b,c):\n",
    "#     assert x==y,print(x,y)\n",
    "# exit()\n",
    "\n",
    "save_model = args.save_model\n",
    "del args.save_model\n",
    "lr = args.lr\n",
    "n_epochs = args.n_epochs\n",
    "batch_size = args.batch_size\n",
    "num_beams = args.num_beams\n",
    "\n",
    "length_penalty = args.length_penalty\n",
    "if isinstance(args.decoder_type, str) and args.decoder_type.lower() == 'none':\n",
    "    args.decoder_type = None\n",
    "decoder_type = args.decoder_type\n",
    "target_type = args.target_type\n",
    "bart_name = args.bart_name\n",
    "schedule = args.schedule\n",
    "use_encoder_mlp = args.use_encoder_mlp\n",
    "\n",
    "fitlog.add_hyper(args)\n",
    "\n",
    "#######hyper\n",
    "#######hyper\n",
    "\n",
    "demo = False\n",
    "if demo:\n",
    "    cache_fn = f\"caches/data_{bart_name}_{dataset_name}_{target_type}_demo.pt\"\n",
    "else:\n",
    "    cache_fn = f\"caches/data_{bart_name}_{dataset_name}_{target_type}.pt\"\n",
    "\n",
    "@cache_results(cache_fn, _refresh=False)\n",
    "def get_data():\n",
    "    pipe = BartNERPipe(tokenizer=bart_name, dataset_name=dataset_name, target_type=target_type)\n",
    "    if dataset_name == 'conll2003':\n",
    "        paths = {'test': \"./data/conll2003/test.txt\",\n",
    "                 'train': \"./data/conll2003/train.txt\",\n",
    "                 'dev': \"./data/conll2003/dev.txt\"}\n",
    "        data_bundle = pipe.process_from_file(paths, demo=demo)\n",
    "    elif dataset_name == 'en-ontonotes':\n",
    "        paths = './data/en-ontonotes/english'\n",
    "        data_bundle = pipe.process_from_file(paths)\n",
    "    else:\n",
    "        data_bundle = pipe.process_from_file(f'./data/{dataset_name}', demo=demo)\n",
    "    return pipe, data_bundle, pipe.tokenizer, pipe.mapping2id\n",
    "print(f'max_len_a:{max_len_a}, max_len:{max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read cache from caches/data_/disk1/wxl/Desktop/DeepKE/example/ner/huggingface/bart-large_re_ace05_word.pt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2050"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe, data_bundle, tokenizer, mapping2id = get_data()\n",
    "ds = data_bundle.get_dataset(\"test\")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastNLP import DataSet,DataSetIter\n",
    "from fastNLP import SequentialSampler\n",
    "sampler = SequentialSampler()\n",
    "batch = DataSetIter(batch_size=1, dataset=ds, sampler=sampler)\n",
    "batch2 = DataSetIter(batch_size=2, dataset=ds, sampler=sampler)\n",
    "# model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set()] \n",
      "target_spans [set()] \n",
      "\n",
      "tensor([[0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set()] \n",
      "target_spans [set()] \n",
      "\n",
      "tensor([[0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set()] \n",
      "target_spans [set()] \n",
      "\n",
      "tensor([[0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set()] \n",
      "target_spans [set()] \n",
      "\n",
      "tensor([[0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set()] \n",
      "target_spans [set()] \n",
      "\n",
      "tensor([[0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set()] \n",
      "target_spans [set()] \n",
      "\n",
      "正确预测个数： 0  错误预测个数： 0  未被预测的正确实体个数： 0\n",
      "{'f': 0.0, 'rec': 0.0, 'pre': 0.0, 'em': 1.0}\n",
      "0   0   0\n",
      "0\n",
      "tensor([[0, 1, 1],\n",
      "        [0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set(), set()] \n",
      "target_spans [set(), set()] \n",
      "\n",
      "tensor([[0, 1, 1],\n",
      "        [0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set(), set()] \n",
      "target_spans [set(), set()] \n",
      "\n",
      "tensor([[0, 1, 1],\n",
      "        [0, 1, 1]], device='cuda:0')\n",
      "pred_spans:  [set(), set()] \n",
      "target_spans [set(), set()] \n",
      "\n",
      "正确预测个数： 0  错误预测个数： 0  未被预测的正确实体个数： 0\n",
      "{'f': 0.0, 'rec': 0.0, 'pre': 0.0, 'em': 1.0}\n",
      "0   0   0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from model.utils import get_span_from_pred\n",
    "label_ids = list(mapping2id.values())\n",
    "metric = Seq2SeqSpanMetric(1, num_labels=len(label_ids), target_type=target_type)\n",
    "model2 = torch.load(\"/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/save_models/re_ace05_3257_1734353331.4825141/best_SequenceGeneratorModel_f_2024-12-16-20-48-57-482293\").to('cuda')\n",
    "model2.eval()\n",
    "max_type_id = len(pipe.mapping2targetid) + 2\n",
    "pred_num = 0\n",
    "all_ent_num = 0\n",
    "false_num = 0\n",
    "true_num = 0\n",
    "false_pred_num = 0\n",
    "false_list = []\n",
    "true_list = [] \n",
    "false_pred_list = []\n",
    "for id,i in enumerate(batch):\n",
    "    #print(id)\n",
    "    src_tokens=i[0][\"src_tokens\"]\n",
    "    src_seq_len=i[0][\"src_seq_len\"]\n",
    "    first=i[0][\"first\"]\n",
    "    tgt_tokens = i[1][\"tgt_tokens\"].to('cuda')\n",
    "    pred = model2.predict(src_tokens.to('cuda'), src_seq_len.to('cuda'), first.to('cuda'))['pred']\n",
    "    pred_spans = [get_span_from_pred(pred[id], max_type_id) for id in range(pred.shape[0])]\n",
    "    pred_spans = [set([str(y) for y in pred_spans[x]]) for x in range(pred.shape[0])]\n",
    "\n",
    "    target_spans = [set([str(list(span)) for span in i[1][\"target_span\"][id]]) for id in range(pred.shape[0])]\n",
    "    \n",
    "    false_spans = [target_spans[id] - pred_spans[id] for id in range(pred.shape[0])] # 没预测出来的span\n",
    "    # print(false_spans)\n",
    "    true_spans = [target_spans[id] & pred_spans[id] for id in range(pred.shape[0])] # 预测出来的span\n",
    "    # x = [list(ss) for ss in pred_spans]\n",
    "    # y = [list(ss) for ss in target_spans]\n",
    "    # for xx in x:\n",
    "    #     xx.sort()\n",
    "    # for yy in y:\n",
    "    #     yy.sort()\n",
    "    # print(\"pred_spans: \",x, \"\\ntarget_spans\",y,\"\\n\")\n",
    "    print(pred)\n",
    "    print(\"pred_spans: \",pred_spans, \"\\ntarget_spans\",target_spans,\"\\n\")\n",
    "    if id > 4:\n",
    "        break\n",
    "    false_pred_spans = [pred_spans[id] - target_spans[id] for id in range(pred.shape[0])]\n",
    "    # x = [list(ss) for ss in false_spans]\n",
    "    # y = [list(ss) for ss in false_pred_spans]\n",
    "    # for xx in x:\n",
    "    #     xx.sort()\n",
    "    # for yy in y:\n",
    "    #     yy.sort()\n",
    "    # print(\"false_spans: \",x, \"\\nfalse_pred_spans\",y,\"\\n\")\n",
    "    # pred_num += sum([len(x[i]) for i in range(pred.shape[0])])\n",
    "    all_ent_num += sum(len(i[1][\"target_span\"][id]) for id in range(pred.shape[0]))\n",
    "    false_num += sum([len(fs) for fs in false_spans])\n",
    "    true_num += sum([len(ts) for ts in true_spans])\n",
    "    false_pred_num += sum(len(fps) for fps in false_pred_spans)\n",
    "    false_list += false_spans\n",
    "    true_list += true_spans\n",
    "    false_pred_list += false_pred_spans\n",
    "    res = metric.evaluate(i[1][\"target_span\"], pred, tgt_tokens)\n",
    "    # print(\"pred: \",pred, \"\\ntarget: \",tgt_tokens,\"\\n\")\n",
    "print(metric.get_metric())\n",
    "print(true_num, \" \", false_num, \" \", false_pred_num)\n",
    "print(all_ent_num)\n",
    "\n",
    "# print(\"=================\")\n",
    "\n",
    "label_ids = list(mapping2id.values())\n",
    "metric = Seq2SeqSpanMetric(1, num_labels=len(label_ids), target_type=target_type)\n",
    "model2.eval()\n",
    "max_type_id = len(pipe.mapping2targetid) + 2\n",
    "pred_num = 0\n",
    "all_ent_num = 0\n",
    "false_num = 0\n",
    "true_num = 0\n",
    "false_pred_num = 0\n",
    "false_list = []\n",
    "true_list = [] \n",
    "false_pred_list = []\n",
    "for id,i in enumerate(batch2):\n",
    "    src_tokens=i[0][\"src_tokens\"]\n",
    "    src_seq_len=i[0][\"src_seq_len\"]\n",
    "    first=i[0][\"first\"]\n",
    "    tgt_tokens = i[1][\"tgt_tokens\"].to('cuda')\n",
    "    pred = model2.predict(src_tokens.to('cuda'), src_seq_len.to('cuda'), first.to('cuda'))['pred']\n",
    "    print(pred)\n",
    "    pred_spans = [get_span_from_pred(pred[id], max_type_id) for id in range(pred.shape[0])]\n",
    "    pred_spans = [set([str(y) for y in pred_spans[x]]) for x in range(pred.shape[0])]\n",
    "\n",
    "    target_spans = [set([str(list(span)) for span in i[1][\"target_span\"][id]]) for id in range(pred.shape[0])]\n",
    "    \n",
    "    false_spans = [target_spans[id] - pred_spans[id] for id in range(pred.shape[0])] # 没预测出来的span\n",
    "    #print(false_spans)\n",
    "    true_spans = [target_spans[id] & pred_spans[id] for id in range(pred.shape[0])] # 没预测出来的span\n",
    "    x = [list(ss) for ss in pred_spans]\n",
    "    y = [list(ss) for ss in target_spans]\n",
    "    for xx in x:\n",
    "        xx.sort()\n",
    "    for yy in y:\n",
    "        yy.sort()\n",
    "    print(\"pred_spans: \",pred_spans, \"\\ntarget_spans\",target_spans,\"\\n\")\n",
    "    if id > 1:\n",
    "        break\n",
    "    false_pred_spans = [pred_spans[id] - target_spans[id] for id in range(pred.shape[0])]\n",
    "    pred_num += sum([len(x[i]) for i in range(pred.shape[0])])\n",
    "    all_ent_num += sum(len(i[1][\"target_span\"][id]) for id in range(pred.shape[0]))\n",
    "    false_num += sum([len(fs) for fs in false_spans])\n",
    "    true_num += sum([len(ts) for ts in true_spans])\n",
    "    false_pred_num += sum(len(fps) for fps in false_pred_spans)\n",
    "    false_list += false_spans\n",
    "    true_list += true_spans\n",
    "    false_pred_list += false_pred_spans\n",
    "    res = metric.evaluate(i[1][\"target_span\"], pred, tgt_tokens)\n",
    "    #print(\"pred: \",pred, \"\\ntarget: \",tgt_tokens,\"\\n\")\n",
    "print(metric.get_metric())\n",
    "print(true_num, \" \", false_num, \" \", false_pred_num)\n",
    "print(all_ent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/save_models/en_ace04_569/best_SequenceGeneratorModel_f_2024-11-26-10-24-31-836238'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/save_models/en_ace04_569/best_SequenceGeneratorModel_f_2024-11-26-10-24-31-836238\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mtype\u001b[39m(model2\u001b[38;5;241m.\u001b[39mseq2seq_model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mencoder_attn)\n",
      "File \u001b[0;32m~/miniconda3/envs/uie/lib/python3.8/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/uie/lib/python3.8/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/uie/lib/python3.8/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/save_models/en_ace04_569/best_SequenceGeneratorModel_f_2024-11-26-10-24-31-836238'"
     ]
    }
   ],
   "source": [
    "model2 = torch.load(\"/disk1/wxl/Desktop/DeepKE/example/baseline/BARTNER/save_models/en_ace04_569/best_SequenceGeneratorModel_f_2024-11-26-10-24-31-836238\").to('cuda')\n",
    "type(model2.seq2seq_model.decoder.decoder.layers[0].encoder_attn)\n",
    "# x = model2.seq2seq_model.decoder.decoder.embed_tokens.weight[2:9]\n",
    "# x = x.unsqueeze(0).expand(48, -1, -1)\n",
    "# x.shape\n",
    "# print(x[0],x[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
